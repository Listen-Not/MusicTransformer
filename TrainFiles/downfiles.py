import os
import re
import requests
from urllib.parse import urljoin, urlparse
from tqdm import tqdm

# å¤šä¸ªç›®æ ‡ç½‘ç«™ï¼Œä»…éœ€å¡«å†™ base_url
TARGET_SITES = [
    "http://piano-midi.de/midicoll.htm",
    # "https://www.kunstderfuge.com/",è¿™ä¸ªè¦ä»˜è´¹
    # æ·»åŠ æ›´å¤šå…¥å£é¡µé¢ï¼š
    # "https://example.com/index.htm",
]

# ä¿å­˜è·¯å¾„ & æ—¥å¿—è·¯å¾„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
save_dir = os.path.join(BASE_DIR, "Input")
log_path = os.path.join(BASE_DIR, "failed_downloads.log")
os.makedirs(save_dir, exist_ok=True)

# æ¸…ç©ºæ—§æ—¥å¿—
with open(log_path, "w") as log_file:
    log_file.write("Failed MIDI Downloads:\n")

# ---------------- å·¥å…·å‡½æ•° ---------------- #


def get_download_base(url):
    parsed = urlparse(url)
    return f"{parsed.scheme}://{parsed.netloc}/"


def fetch(url):
    try:
        response = requests.get(url, timeout=(20, 30))
        response.raise_for_status()
        return response.text
    except Exception as e:
        tqdm.write(f"âŒ Failed to fetch {url}: {e}")
        return None


def extract_subpages(html, base):
    return list(set([urljoin(base, link) for link in re.findall(r'href="([^"]+\.htm[l]?)"', html, re.IGNORECASE)]))


def extract_midi_links(html, base):
    return list(set([urljoin(base, link) for link in re.findall(r'href="([^"]+\.(?:mid|midi))"', html, re.IGNORECASE)]))


def download_file(url, save_dir, log_path):
    file_name = os.path.basename(urlparse(url).path)
    file_path = os.path.join(save_dir, file_name)
    if os.path.exists(file_path):
        return

    try:
        r = requests.get(url, timeout=15)
        r.raise_for_status()
        with open(file_path, "wb") as f:
            f.write(r.content)
    except Exception as e:
        with open(log_path, "a") as log_file:
            log_file.write(f"{url}  # {e}\n")


# ---------------- ä¸»æµç¨‹ ---------------- #

for base_url in TARGET_SITES:
    download_base = get_download_base(base_url)

    print(f"\nğŸŒ Fetching base page: {base_url}")
    main_html = fetch(base_url)
    if not main_html:
        continue

    # æå–å­é¡µé¢
    subpages = extract_subpages(main_html, download_base)
    print(f"ğŸ”— Found {len(subpages)} subpages from {base_url}")

    # æå–æ‰€æœ‰ MIDI é“¾æ¥
    all_midi_links = []
    for subpage in tqdm(subpages, desc=f"ğŸ” Scanning subpages of {urlparse(base_url).netloc}", leave=False):
        html = fetch(subpage)
        if html:
            midi_links = extract_midi_links(html, subpage)
            all_midi_links.extend(midi_links)

    # å»é‡
    all_midi_links = list(set(all_midi_links))
    print(f"ğŸ¼ Found {len(all_midi_links)} MIDI files in {base_url}")

    # ä¸‹è½½
    for url in tqdm(
        all_midi_links, desc=f"ğŸ¹ Downloading from {urlparse(base_url).netloc}", unit="file", unit_scale=True
    ):
        download_file(url, save_dir, log_path)

print("\nâœ… All downloads complete.")
print(f"ğŸµ MIDI files saved to: {save_dir}")
print(f"ğŸ“„ Failed downloads logged in: {log_path}")
